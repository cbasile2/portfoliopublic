{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482d3a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Create feature and target arrays\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Checking class balance\n",
    "class_balance = pd.Series(y).value_counts(normalize=True)\n",
    "print(f\"Class balance: \\n{class_balance}\")\n",
    "\n",
    "# Split into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42)\n",
    "\n",
    "# KFold cross-validation\n",
    "kf = KFold(n_splits=3)\n",
    "\n",
    "def evaluate(model, X_train, y_train, X_test, y_test):\n",
    "    # Fit the model and make predictions\n",
    "    model.fit(X_train, y_train)\n",
    "    train_preds = model.predict(X_train)\n",
    "    test_preds = model.predict(X_test)\n",
    "\n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        \"accuracy_train\": accuracy_score(y_train, train_preds),\n",
    "        \"accuracy_test\": accuracy_score(y_test, test_preds),\n",
    "        \"f1_train\": f1_score(y_train, train_preds, average='macro'),\n",
    "        \"f1_test\": f1_score(y_test, test_preds, average='macro'),\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def objective_xgboost(trial):\n",
    "    # Suggest hyperparameters\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 0.1)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 5, 20)\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 100, 200)\n",
    "    min_child_weight = trial.suggest_int('min_child_weight', 1, 6)\n",
    "    \n",
    "    model = XGBClassifier(learning_rate=learning_rate, max_depth=max_depth, n_estimators=n_estimators, \n",
    "                          min_child_weight=min_child_weight, random_state=42, objective='multi:softmax', num_class=3)\n",
    "\n",
    "    metrics = evaluate(model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "    # Set additional attributes to trial\n",
    "    for key, value in metrics.items():\n",
    "        trial.set_user_attr(key, value)\n",
    "    \n",
    "    return metrics[\"accuracy_test\"]\n",
    "\n",
    "# Create the optuna study\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective_xgboost, n_trials=10)\n",
    "\n",
    "# Print the result\n",
    "best_trials = study.get_trials(deepcopy=False, states=[optuna.trial.TrialState.COMPLETE])[:5]\n",
    "\n",
    "# Collecting the results to a dataframe\n",
    "results_df = pd.DataFrame()\n",
    "for i, trial in enumerate(best_trials):\n",
    "    results_df.loc[i, 'Trial'] = trial.number\n",
    "    for key in trial.user_attrs:\n",
    "        results_df.loc[i, key] = trial.user_attrs[key]\n",
    "    results_df.loc[i, 'Hyperparameters'] = str(trial.params)\n",
    "\n",
    "print(results_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
