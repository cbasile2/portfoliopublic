{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b50ffdbd",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (4247374648.py, line 68)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/3l/yn2hqvzn44x1knqzb_2538xw0000gn/T/ipykernel_43087/4247374648.py\"\u001b[0;36m, line \u001b[0;32m68\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "# Define the custom gym environment for the boxing match\n",
    "class BoxingMatchEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        # Define the action and observation space\n",
    "        num_actions = 2  # Punch and Dodge\n",
    "        num_observations = 6  # Assuming 6 features for partial observation\n",
    "        self.action_space = gym.spaces.Discrete(num_actions)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(num_observations,), dtype=np.float32)\n",
    "        \n",
    "        # Initialize the state of the boxing match\n",
    "        self.state = self.reset()\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Implement the transition function based on the action taken\n",
    "        # Update the state of the boxing match\n",
    "        \n",
    "        # Calculate the reward based on the action and resulting state\n",
    "        reward = calculate_reward(action, self.state)\n",
    "        \n",
    "        # Check if the episode is done (e.g., a knockout or time limit reached)\n",
    "        done = check_if_episode_done(self.state)\n",
    "        \n",
    "        # Update the observation based on the new state (partially observable environment)\n",
    "        observation = get_partial_observation(self.state)\n",
    "        \n",
    "        return observation, reward, done, {}\n",
    "    \n",
    "    def reset(self):\n",
    "        # Reset the state of the boxing match for a new episode\n",
    "        # Define the initial state, including positions and velocities of both boxers\n",
    "        initial_state = np.array([...])  # Add the initial state here\n",
    "        \n",
    "        return get_partial_observation(self.state)\n",
    "\n",
    "# Define the reinforcement learning agent\n",
    "class BoxingAgent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        # Initialize your RL algorithm and policy here (e.g., Q-learning, DQN, PPO)\n",
    "    \n",
    "    def train(self, num_episodes):\n",
    "        for episode in range(num_episodes):\n",
    "            done = False\n",
    "            observation = self.env.reset()\n",
    "            \n",
    "            while not done:\n",
    "                # Use your RL algorithm to select an action based on the observation\n",
    "                action = self.select_action(observation)\n",
    "                \n",
    "                # Take the action in the environment and get the next observation, reward, and done flag\n",
    "                next_observation, reward, done, _ = self.env.step(action)\n",
    "                \n",
    "                # Use the observation, action, reward, and next observation to update your RL algorithm\n",
    "                \n",
    "                # Update the current observation\n",
    "                observation = next_observation\n",
    "    \n",
    "    def select_action(self, observation):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "387512e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/3l/yn2hqvzn44x1knqzb_2538xw0000gn/T/ipykernel_43087/920317557.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrandom_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Both agents take random actions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mtotal_rewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "# Define the custom gym environment for the coin collection game\n",
    "class CoinCollectionEnv(gym.Env):\n",
    "    def __init__(self, num_coins=10, max_steps=100):\n",
    "        # Define the action and observation space\n",
    "        num_actions = 4  # Up, Down, Left, Right\n",
    "        self.action_space = gym.spaces.Discrete(num_actions)\n",
    "        self.observation_space = gym.spaces.Discrete(num_coins + 1)  # Number of coins collected\n",
    "        \n",
    "        # Initialize the state of the environment\n",
    "        self.num_coins = num_coins\n",
    "        self.max_steps = max_steps\n",
    "        self.coins = None\n",
    "        self.state = self.reset()\n",
    "    \n",
    "    def step(self, actions):\n",
    "        # Implement the transition function based on the actions taken by both agents\n",
    "        # Update the state of the environment\n",
    "        \n",
    "        # Calculate the rewards based on the actions and resulting state\n",
    "        rewards = [calculate_reward(action, self.coins) for action in actions]\n",
    "        \n",
    "        # Check if the episode is done (e.g., max steps reached)\n",
    "        done = self.state[-1] >= self.max_steps\n",
    "        \n",
    "        # Update the observation based on the new state (number of coins collected)\n",
    "        observation = self.state[0]\n",
    "        \n",
    "        return observation, rewards, done, {}\n",
    "    \n",
    "    def reset(self):\n",
    "        # Reset the state of the environment for a new episode\n",
    "        # Initialize the coins at random positions for each episode\n",
    "        self.coins = self.generate_coins(self.num_coins)\n",
    "        # Reset the number of steps and the number of coins collected\n",
    "        self.state = np.array([0, 0])  # [Number of coins collected, Number of steps]\n",
    "        \n",
    "        return self.state[0]  # Return the initial observation (number of coins collected)\n",
    "    \n",
    "    def generate_coins(self, num_coins):\n",
    "        # Helper function to generate coins at random positions\n",
    "        coins = []\n",
    "        for _ in range(num_coins):\n",
    "            x = np.random.randint(0, 5)  # Assuming a 5x5 grid, you can adjust this as needed\n",
    "            y = np.random.randint(0, 5)\n",
    "            coins.append((x, y))\n",
    "        return coins\n",
    "\n",
    "def calculate_reward(action, coins):\n",
    "    # Helper function to calculate the reward based on the action taken and the coins collected\n",
    "    # You can define the reward based on the specific rules of the game\n",
    "    # For example, if the agent moves to a cell with a coin, it gets a positive reward, otherwise zero reward\n",
    "    x, y = coins[action]  # Assume action corresponds to the index of the coin in the coins list\n",
    "    if (x, y) in coins:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "# Create the custom gym environment\n",
    "env = CoinCollectionEnv(num_coins=10, max_steps=100)\n",
    "\n",
    "# Define two simple agents with random actions\n",
    "def random_agent(observation):\n",
    "    return np.random.choice([0, 1, 2, 3])  # Randomly choose an action (Up, Down, Left, Right)\n",
    "\n",
    "# Run the game for a fixed number of steps\n",
    "num_steps = 50\n",
    "total_rewards = [0, 0]  # Initialize the total rewards for each agent\n",
    "\n",
    "for _ in range(num_steps):\n",
    "    actions = [random_agent(env.state[0]), random_agent(env.state[0])]  # Both agents take random actions\n",
    "    observation, rewards, done, _ = env.step(actions)\n",
    "    total_rewards[0] += rewards[0]\n",
    "    total_rewards[1] += rewards[1]\n",
    "    \n",
    "    if done:\n",
    "        break\n",
    "\n",
    "# Determine the winner based on the total number of coins collected\n",
    "winner = np.argmax(total_rewards)\n",
    "print(\"Total rewards for Agent 1:\", total_rewards[0])\n",
    "print(\"Total rewards for Agent 2:\", total_rewards[1])\n",
    "print(\"Winner: Agent\", winner + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c01e701",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
