{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90cde386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 0.6912\n",
      "Epoch [20/100], Loss: 0.6873\n",
      "Epoch [30/100], Loss: 0.6855\n",
      "Epoch [40/100], Loss: 0.6843\n",
      "Epoch [50/100], Loss: 0.6833\n",
      "Epoch [60/100], Loss: 0.6823\n",
      "Epoch [70/100], Loss: 0.6813\n",
      "Epoch [80/100], Loss: 0.6803\n",
      "Epoch [90/100], Loss: 0.6792\n",
      "Epoch [100/100], Loss: 0.6781\n"
     ]
    }
   ],
   "source": [
    "# Import the required libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "# Define the model architecture\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "# Define the hyperparameters\n",
    "input_size = 10     # Input size (Number of features)\n",
    "hidden_size = 5     # Number of nodes at hidden layer\n",
    "num_classes = 2     # Number of output classes. As it's a binary classification, this is 2.\n",
    "num_epochs = 100    # Number of times which the entire dataset is passed throughout the model\n",
    "batch_size = 100    # The size of input data took for one iteration\n",
    "learning_rate = 0.001  # The speed of convergence\n",
    "\n",
    "# Create the model\n",
    "model = SimpleNN(input_size, hidden_size, num_classes)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "# Dummy data for training\n",
    "# In practice, you will want to load your real data here\n",
    "x_train = torch.rand(batch_size, input_size) \n",
    "y_train = torch.randint(0, 2, (batch_size,))\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(x_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "\n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'model.ckpt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96084af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/200], Loss: 1.5047\n",
      "Epoch [20/200], Loss: 1.4562\n",
      "Epoch [30/200], Loss: 1.4623\n",
      "Epoch [40/200], Loss: 1.4407\n",
      "Epoch [50/200], Loss: 1.3256\n",
      "Epoch [60/200], Loss: 1.3436\n",
      "Epoch [70/200], Loss: 1.4363\n",
      "Epoch [80/200], Loss: 1.3718\n",
      "Epoch [90/200], Loss: 1.3510\n",
      "Epoch [100/200], Loss: 1.2592\n",
      "Epoch [110/200], Loss: 1.3496\n",
      "Epoch [120/200], Loss: 1.3475\n",
      "Epoch [130/200], Loss: 1.3707\n",
      "Epoch [140/200], Loss: 1.2200\n",
      "Epoch [150/200], Loss: 1.2480\n",
      "Epoch [160/200], Loss: 1.2601\n",
      "Epoch [170/200], Loss: 1.1345\n",
      "Epoch [180/200], Loss: 1.2264\n",
      "Epoch [190/200], Loss: 1.2000\n",
      "Epoch [200/200], Loss: 1.2216\n",
      "Total number of parameters is: 85\n"
     ]
    }
   ],
   "source": [
    "# Import the required libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "# Define the model architecture\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, num_classes, dropout_p):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.fc3 = nn.Linear(hidden_size2, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "        # Initialization\n",
    "        nn.init.kaiming_normal_(self.fc1.weight)\n",
    "        nn.init.kaiming_normal_(self.fc2.weight)\n",
    "        nn.init.kaiming_normal_(self.fc3.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "# Define the hyperparameters\n",
    "input_size = 10     # Input size (Number of features)\n",
    "hidden_size1 = 5    # Number of nodes at hidden layer 1\n",
    "hidden_size2 = 3    # Number of nodes at hidden layer 2\n",
    "num_classes = 3     # Number of output classes. This is now 3.\n",
    "num_epochs = 200    # Number of times which the entire dataset is passed throughout the model\n",
    "batch_size = 100    # The size of input data took for one iteration\n",
    "learning_rate = 0.001  # The speed of convergence\n",
    "dropout_p = 0.5    # Dropout probability\n",
    "\n",
    "# Create the model\n",
    "model = SimpleNN(input_size, hidden_size1, hidden_size2, num_classes, dropout_p)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "# Dummy data for training\n",
    "x_train = torch.rand(batch_size, input_size) \n",
    "y_train = torch.randint(0, 3, (batch_size,))\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(x_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "\n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'model.ckpt')\n",
    "\n",
    "# Counting the number of parameters\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total number of parameters is: {total_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a451966d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 1.0215\n",
      "Epoch [20/100], Loss: 1.2673\n",
      "Epoch [30/100], Loss: 0.8684\n",
      "Epoch [40/100], Loss: 0.6529\n",
      "Epoch [50/100], Loss: 0.8320\n",
      "Epoch [60/100], Loss: 0.7534\n",
      "Epoch [70/100], Loss: 0.8708\n",
      "Epoch [80/100], Loss: 0.8122\n",
      "Epoch [90/100], Loss: 0.6901\n",
      "Epoch [100/100], Loss: 0.7251\n",
      "Total number of parameters is: 77\n",
      "Total time for training: 0.08755207061767578 seconds\n"
     ]
    }
   ],
   "source": [
    "# Import the required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "# Define the model architecture\n",
    "class RegressionNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size, dropout_p):\n",
    "        super(RegressionNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "        # Initialization\n",
    "        nn.init.kaiming_normal_(self.fc1.weight)\n",
    "        nn.init.kaiming_normal_(self.fc2.weight)\n",
    "        nn.init.kaiming_normal_(self.fc3.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "# Define the hyperparameters\n",
    "input_size = 10     # Input size (Number of features)\n",
    "hidden_size1 = 5    # Number of nodes at hidden layer 1\n",
    "hidden_size2 = 3    # Number of nodes at hidden layer 2\n",
    "output_size = 1     # Output size. As it's a regression task, this is 1.\n",
    "num_epochs = 100    # Number of times which the entire dataset is passed throughout the model\n",
    "batch_size = 100    # The size of input data took for one iteration\n",
    "learning_rate = 0.001  # The speed of convergence\n",
    "dropout_p = 0.5    # Dropout probability\n",
    "weight_decay = 0.01 # Weight decay for L2 regularization\n",
    "\n",
    "# Create the model\n",
    "model = RegressionNN(input_size, hidden_size1, hidden_size2, output_size, dropout_p)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# Dummy data for training\n",
    "x_train = torch.rand(batch_size, input_size)\n",
    "y_train = torch.rand(batch_size, output_size)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(x_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "\n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'model.ckpt')\n",
    "\n",
    "# Counting the number of parameters\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total number of parameters is: {total_params}\")\n",
    "\n",
    "# Print the total time for training\n",
    "print(f\"Total time for training: {end_time - start_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f133585",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-16 17:34:08.629995: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 5)                 55        \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 5)                 0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 18        \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 3)                 0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 4         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 77\n",
      "Trainable params: 77\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 2.6156\n",
      "Epoch 2/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 2.2874\n",
      "Epoch 3/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.6578\n",
      "Epoch 4/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 2.0073\n",
      "Epoch 5/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 2.2542\n",
      "Epoch 6/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.8232\n",
      "Epoch 7/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.0378\n",
      "Epoch 8/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 1.5165\n",
      "Epoch 9/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.2459\n",
      "Epoch 10/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.8583\n",
      "Epoch 11/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.7249\n",
      "Epoch 12/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.4559\n",
      "Epoch 13/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.0396\n",
      "Epoch 14/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.5728\n",
      "Epoch 15/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.6256\n",
      "Epoch 16/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.6455\n",
      "Epoch 17/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.6411\n",
      "Epoch 18/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.5174\n",
      "Epoch 19/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.5330\n",
      "Epoch 20/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.8120\n",
      "Epoch 21/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.1906\n",
      "Epoch 22/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.5983\n",
      "Epoch 23/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.3727\n",
      "Epoch 24/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.2724\n",
      "Epoch 25/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.4445\n",
      "Epoch 26/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.2957\n",
      "Epoch 27/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.3653\n",
      "Epoch 28/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.1898\n",
      "Epoch 29/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.3897\n",
      "Epoch 30/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.0944\n",
      "Epoch 31/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.3838\n",
      "Epoch 32/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.3062\n",
      "Epoch 33/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.4975\n",
      "Epoch 34/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.1167\n",
      "Epoch 35/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.2993\n",
      "Epoch 36/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.2085\n",
      "Epoch 37/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.4451\n",
      "Epoch 38/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.0109\n",
      "Epoch 39/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.3127\n",
      "Epoch 40/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.2890\n",
      "Epoch 41/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.1431\n",
      "Epoch 42/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.4102\n",
      "Epoch 43/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.0878\n",
      "Epoch 44/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.2235\n",
      "Epoch 45/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.1538\n",
      "Epoch 46/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.2696\n",
      "Epoch 47/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.0183\n",
      "Epoch 48/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.1349\n",
      "Epoch 49/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.3049\n",
      "Epoch 50/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.0695\n",
      "Epoch 51/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.2974\n",
      "Epoch 52/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.0266\n",
      "Epoch 53/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.0678\n",
      "Epoch 54/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.1011\n",
      "Epoch 55/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.8080\n",
      "Epoch 56/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.8360\n",
      "Epoch 57/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.9668\n",
      "Epoch 58/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.9691\n",
      "Epoch 59/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.1956\n",
      "Epoch 60/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.1549\n",
      "Epoch 61/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.2112\n",
      "Epoch 62/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.9088\n",
      "Epoch 63/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.9048\n",
      "Epoch 64/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.9846\n",
      "Epoch 65/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 1.3099\n",
      "Epoch 66/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.9531\n",
      "Epoch 67/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.0713\n",
      "Epoch 68/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.0482\n",
      "Epoch 69/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.0531\n",
      "Epoch 70/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.0564\n",
      "Epoch 71/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.0704\n",
      "Epoch 72/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.9122\n",
      "Epoch 73/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.9674\n",
      "Epoch 74/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.1256\n",
      "Epoch 75/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.8114\n",
      "Epoch 76/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.8400\n",
      "Epoch 77/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.2428\n",
      "Epoch 78/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.0389\n",
      "Epoch 79/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.8445\n",
      "Epoch 80/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.0483\n",
      "Epoch 81/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.9599\n",
      "Epoch 82/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.8765\n",
      "Epoch 83/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.8239\n",
      "Epoch 84/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.0288\n",
      "Epoch 85/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.0045\n",
      "Epoch 86/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.9863\n",
      "Epoch 87/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.8557\n",
      "Epoch 88/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.9503\n",
      "Epoch 89/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.0912\n",
      "Epoch 90/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.8537\n",
      "Epoch 91/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.7337\n",
      "Epoch 92/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.9592\n",
      "Epoch 93/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.8542\n",
      "Epoch 94/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.8056\n",
      "Epoch 95/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.8435\n",
      "Epoch 96/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.0042\n",
      "Epoch 97/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.0935\n",
      "Epoch 98/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.6592\n",
      "Epoch 99/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.8885\n",
      "Epoch 100/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.0741\n",
      "Total number of parameters is: 77\n",
      "Total time for training: 1.3002560138702393 seconds\n"
     ]
    }
   ],
   "source": [
    "# Import the required libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import time\n",
    "\n",
    "# Define the model architecture\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(5, activation='relu', input_shape=(10,), kernel_initializer='he_normal'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(3, activation='sigmoid', kernel_initializer='he_normal'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(1, kernel_initializer='he_normal')\n",
    "])\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, decay=0.01)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='mean_squared_error')\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()\n",
    "\n",
    "# Dummy data for training\n",
    "x_train = tf.random.uniform((100, 10))\n",
    "y_train = tf.random.uniform((100, 1))\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Training the model\n",
    "history = model.fit(x_train, y_train, epochs=100, verbose=1)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# Counting the number of parameters\n",
    "total_params = model.count_params()\n",
    "print(f\"Total number of parameters is: {total_params}\")\n",
    "\n",
    "# Print the total time for training\n",
    "print(f\"Total time for training: {end_time - start_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebbc771e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 2.2931\n",
      "Epoch [20/100], Loss: 2.2917\n",
      "Epoch [30/100], Loss: 2.2901\n",
      "Epoch [40/100], Loss: 2.2884\n",
      "Epoch [50/100], Loss: 2.2867\n",
      "Epoch [60/100], Loss: 2.2850\n",
      "Epoch [70/100], Loss: 2.2834\n",
      "Epoch [80/100], Loss: 2.2818\n",
      "Epoch [90/100], Loss: 2.2802\n",
      "Epoch [100/100], Loss: 2.2786\n",
      "Total number of parameters is: 62006\n",
      "Total time for training: 1.261061191558838 seconds\n"
     ]
    }
   ],
   "source": [
    "# Import the required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "# Import the required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "# Define the model architecture\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5) # input channels = 3, output channels=6, kernel size =5\n",
    "        self.pool = nn.MaxPool2d(2, 2)  # kernel size =2, stride=2\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5) \n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # Fully connected layer\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)  # Reshape before passing to fc layer\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Create the model\n",
    "model = SimpleCNN()\n",
    "\n",
    "# Define the loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Dummy data for training\n",
    "# In practice, you will want to load your real data here\n",
    "x_train = torch.rand(100, 3, 32, 32)\n",
    "y_train = torch.randint(0, 10, (100,))\n",
    "\n",
    "# Training loop placeholder\n",
    "# In practice, replace with your actual training loop\n",
    "for epoch in range(100):  \n",
    "    outputs = model(x_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# Counting the number of parameters\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total number of parameters is: {total_params}\")\n",
    "\n",
    "# Print the total time for training\n",
    "print(f\"Total time for training: {end_time - start_time} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ca2a917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters is: 14113\n",
      "Total time for training: 0.479403018951416 seconds\n"
     ]
    }
   ],
   "source": [
    "# Import the required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "# Define the model architecture\n",
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(SimpleLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device) \n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        \n",
    "        out = out[:, -1, :]  # Get the last output of the sequence\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Define the hyperparameters\n",
    "input_size = 10  # Input size of LSTM, depends on your data\n",
    "hidden_size = 32  # Size of LSTM hidden state\n",
    "num_layers = 2  # Number of LSTM layers\n",
    "num_classes = 1  # Output size, depends on your task\n",
    "num_epochs = 100  # Number of epochs for training\n",
    "learning_rate = 0.001  # Learning rate\n",
    "\n",
    "# Create the model\n",
    "model = SimpleLSTM(input_size, hidden_size, num_layers, num_classes)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()  # Change this to a suitable loss function for your task\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Dummy data for training\n",
    "# In practice, you will want to load your real data here\n",
    "x_train = torch.rand(100, 5, input_size)  # 100 sequences, each of length 5, each with 10 features\n",
    "y_train = torch.rand(100, num_classes)\n",
    "\n",
    "# Training loop placeholder\n",
    "# In practice, replace with your actual training loop\n",
    "for epoch in range(num_epochs):\n",
    "    outputs = model(x_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# Counting the number of parameters\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total number of parameters is: {total_params}\")\n",
    "\n",
    "# Print the total time for training\n",
    "print(f\"Total time for training: {end_time - start_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d6473d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 0.1032\n",
      "Epoch [20/100], Loss: 0.0839\n",
      "Epoch [30/100], Loss: 0.0838\n",
      "Epoch [40/100], Loss: 0.0804\n",
      "Epoch [50/100], Loss: 0.0763\n",
      "Epoch [60/100], Loss: 0.0735\n",
      "Epoch [70/100], Loss: 0.0711\n",
      "Epoch [80/100], Loss: 0.0688\n",
      "Epoch [90/100], Loss: 0.0663\n",
      "Epoch [100/100], Loss: 0.0634\n",
      "Total number of parameters is: 3553\n",
      "Total time for training: 0.2569000720977783 seconds\n"
     ]
    }
   ],
   "source": [
    "# Import the required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "# Define the model architecture\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)  # Simple RNN layer\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device) \n",
    "        \n",
    "        out, _ = self.rnn(x, h0)  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        \n",
    "        out = out[:, -1, :]  # Get the last output of the sequence\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Define the hyperparameters\n",
    "input_size = 10  # Input size of RNN, depends on your data\n",
    "hidden_size = 32  # Size of RNN hidden state\n",
    "num_layers = 2  # Number of RNN layers\n",
    "num_classes = 1  # Output size, depends on your task\n",
    "num_epochs = 100  # Number of epochs for training\n",
    "learning_rate = 0.001  # Learning rate\n",
    "\n",
    "# Create the model\n",
    "model = SimpleRNN(input_size, hidden_size, num_layers, num_classes)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()  # Change this to a suitable loss function for your task\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Dummy data for training\n",
    "# In practice, you will want to load your real data here\n",
    "x_train = torch.rand(100, 5, input_size)  # 100 sequences, each of length 5, each with 10 features\n",
    "y_train = torch.rand(100, num_classes)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    outputs = model(x_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# Counting the number of parameters\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total number of parameters is: {total_params}\")\n",
    "\n",
    "# Print the total time for training\n",
    "print(f\"Total time for training: {end_time - start_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb0ba07d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 1.0880\n",
      "Epoch [20/100], Loss: 1.0863\n",
      "Epoch [30/100], Loss: 1.0825\n",
      "Epoch [40/100], Loss: 1.0710\n",
      "Epoch [50/100], Loss: 1.0391\n",
      "Epoch [60/100], Loss: 1.0111\n",
      "Epoch [70/100], Loss: 0.9692\n",
      "Epoch [80/100], Loss: 0.9150\n",
      "Epoch [90/100], Loss: 0.8611\n",
      "Epoch [100/100], Loss: 0.8045\n",
      "Total number of parameters is: 14179\n",
      "Total time for training: 0.5224368572235107 seconds\n"
     ]
    }
   ],
   "source": [
    "# Import the required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "# Define the model architecture\n",
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(SimpleLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device) \n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        \n",
    "        out = out[:, -1, :]  # Get the last output of the sequence\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Define the hyperparameters\n",
    "input_size = 10  # Input size of LSTM, depends on your data\n",
    "hidden_size = 32  # Size of LSTM hidden state\n",
    "num_layers = 2  # Number of LSTM layers\n",
    "num_classes = 3  # Output size, depends on your task\n",
    "num_epochs = 100  # Number of epochs for training\n",
    "learning_rate = 0.001  # Learning rate\n",
    "\n",
    "# Create the model\n",
    "model = SimpleLSTM(input_size, hidden_size, num_layers, num_classes)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Change this to a suitable loss function for your task\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Dummy data for training\n",
    "# In practice, you will want to load your real data here\n",
    "x_train = torch.rand(100, 5, input_size)  # 100 sequences, each of length 5, each with 10 features\n",
    "y_train = torch.randint(0, 3, (100,))  # Dummy target labels\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    outputs = model(x_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# Counting the number of parameters\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total number of parameters is: {total_params}\")\n",
    "\n",
    "# Print the total time for training\n",
    "print(f\"Total time for training: {end_time - start_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "282611f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 0.7730\n",
      "Epoch [20/100], Loss: 0.7006\n",
      "Epoch [30/100], Loss: 0.6961\n",
      "Epoch [40/100], Loss: 0.6889\n",
      "Epoch [50/100], Loss: 0.6901\n",
      "Epoch [60/100], Loss: 0.6888\n",
      "Epoch [70/100], Loss: 0.6789\n",
      "Epoch [80/100], Loss: 0.6841\n",
      "Epoch [90/100], Loss: 0.6853\n",
      "Epoch [100/100], Loss: 0.6871\n",
      "Total number of parameters is: 9458178\n",
      "Total time for training: 42.05142402648926 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads, num_layers, num_classes):\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "        transformer_layer = nn.TransformerEncoderLayer(d_model=input_dim, nhead=num_heads)\n",
    "        self.transformer = nn.TransformerEncoder(transformer_layer, num_layers=num_layers)\n",
    "        self.classifier = nn.Linear(input_dim, num_classes)\n",
    "\n",
    "    def forward(self, src):\n",
    "        trans_output = self.transformer(src)\n",
    "        output = self.classifier(trans_output[:, -1, :])  # Get the last output for classification\n",
    "        return output\n",
    "\n",
    "# Define the hyperparameters\n",
    "input_dim = 512  # Input size of transformer, depends on your data\n",
    "num_heads = 8  # Number of attention heads\n",
    "num_layers = 3  # Number of transformer layers\n",
    "num_classes = 2  # Output size, depends on your task\n",
    "num_epochs = 100  # Number of epochs for training\n",
    "learning_rate = 0.001  # Learning rate\n",
    "\n",
    "# Create the model\n",
    "model = TransformerClassifier(input_dim, num_heads, num_layers, num_classes)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Change this to a suitable loss function for your task\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Dummy data for training\n",
    "# In practice, you will want to load your real data here\n",
    "x_train = torch.rand(100, 5, input_dim)  # 100 sequences, each of length 5, each with 512 features\n",
    "y_train = torch.randint(0, 2, (100,))  # Dummy target labels\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    outputs = model(x_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# Counting the number of parameters\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total number of parameters is: {total_params}\")\n",
    "\n",
    "# Print the total time for training\n",
    "print(f\"Total time for training: {end_time - start_time} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f2acce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc5bd1fe58b9481e8ceca3c892432292",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37c4a5a6c58941c8b8492c1335bdd64c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb68edd48cc541a59c316aea1d10ffbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fefd30b67fa46ec8e9293bdd5bf54de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Epoch[1/10], Loss: 3996.6423\n",
      "Epoch[2/10], Loss: 3499.3037\n",
      "Epoch[3/10], Loss: 3520.3003\n",
      "Epoch[4/10], Loss: 3431.1226\n",
      "Epoch[5/10], Loss: 3595.9377\n",
      "Epoch[6/10], Loss: 3576.4504\n",
      "Epoch[7/10], Loss: 3568.4473\n",
      "Epoch[8/10], Loss: 3385.6123\n",
      "Epoch[9/10], Loss: 3238.4417\n",
      "Epoch[10/10], Loss: 3356.6611\n",
      "Total time for training: 133.97194814682007 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc21 = nn.Linear(hidden_dim, latent_dim)  # mu layer\n",
    "        self.fc22 = nn.Linear(hidden_dim, latent_dim)  # logvar layer\n",
    "        \n",
    "        # Decoder\n",
    "        self.fc3 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = torch.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = torch.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 784))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "# Hyperparameters\n",
    "input_dim = 784\n",
    "hidden_dim = 400\n",
    "latent_dim = 20\n",
    "lr = 0.001\n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "# Load Data\n",
    "dataset = datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Model\n",
    "model = VAE(input_dim, hidden_dim, latent_dim)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Loss function\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = nn.functional.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "# Training loop\n",
    "start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        img, _ = batch\n",
    "        img = img.view(img.size(0), -1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward\n",
    "        recon_batch, mu, logvar = model(img)\n",
    "        loss = loss_function(recon_batch, img, mu, logvar)\n",
    "        \n",
    "        # Backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print('Epoch[{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Total time for training: {end_time - start_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "779faca0",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable float object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/3l/yn2hqvzn44x1knqzb_2538xw0000gn/T/ipykernel_43490/531906489.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Solve the ODE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0msolution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0modeint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlotka_volterra\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Plot the solution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/scipy/integrate/odepack.py\u001b[0m in \u001b[0;36modeint\u001b[0;34m(func, y0, t, args, Dfun, col_deriv, full_output, ml, mu, rtol, atol, tcrit, h0, hmax, hmin, ixpr, mxstep, mxhnil, mxordn, mxords, printmessg, tfirst)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0my0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m     output = _odepack.odeint(func, y0, t, args, Dfun, col_deriv, ml, mu,\n\u001b[0m\u001b[1;32m    242\u001b[0m                              \u001b[0mfull_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtcrit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m                              \u001b[0mixpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmxstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmxhnil\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmxordn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmxords\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/3l/yn2hqvzn44x1knqzb_2538xw0000gn/T/ipykernel_43490/531906489.py\u001b[0m in \u001b[0;36mlotka_volterra\u001b[0;34m(t, z, alpha, beta, delta, gamma)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Lotka-Volterra system\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlotka_volterra\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mdx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mdy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable float object"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import odeint\n",
    "import numpy as np\n",
    "\n",
    "# Lotka-Volterra system\n",
    "def lotka_volterra(t, z, alpha=1.0, beta=0.1, delta=0.1, gamma=1.0):\n",
    "    x, y = z\n",
    "    dx = alpha*x - beta*x*y\n",
    "    dy = delta*x*y - gamma*y\n",
    "    return [dx, dy]\n",
    "\n",
    "# Time points\n",
    "t = torch.linspace(0, 10, 1000)\n",
    "\n",
    "# Initial conditions: 10 rabbits and 5 foxes\n",
    "z0 = torch.tensor([10.0, 5.0])\n",
    "\n",
    "# Solve the ODE\n",
    "solution = odeint(lotka_volterra, z0, t)\n",
    "\n",
    "# Plot the solution\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(t, solution)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Population Size')\n",
    "plt.legend(['Rabbits', 'Foxes'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d32664b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/3l/yn2hqvzn44x1knqzb_2538xw0000gn/T/ipykernel_43490/1913574623.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Solve the ODE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0msolution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscipy_odeint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlotka_volterra\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Convert the solution to torch tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/scipy/integrate/odepack.py\u001b[0m in \u001b[0;36modeint\u001b[0;34m(func, y0, t, args, Dfun, col_deriv, full_output, ml, mu, rtol, atol, tcrit, h0, hmax, hmin, ixpr, mxstep, mxhnil, mxordn, mxords, printmessg, tfirst)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0my0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m     output = _odepack.odeint(func, y0, t, args, Dfun, col_deriv, ml, mu,\n\u001b[0m\u001b[1;32m    242\u001b[0m                              \u001b[0mfull_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtcrit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m                              \u001b[0mixpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmxstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmxhnil\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmxordn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmxords\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/3l/yn2hqvzn44x1knqzb_2538xw0000gn/T/ipykernel_43490/1913574623.py\u001b[0m in \u001b[0;36mlotka_volterra\u001b[0;34m(t, z, alpha, beta, delta, gamma)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Lotka-Volterra system\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlotka_volterra\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mdx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mdy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchdiffeq import odeint\n",
    "from scipy.integrate import odeint as scipy_odeint\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Lotka-Volterra system\n",
    "def lotka_volterra(t, z, alpha=1.0, beta=0.1, delta=0.1, gamma=1.0):\n",
    "    x, y = z[0], z[1]\n",
    "    dx = alpha*x - beta*x*y\n",
    "    dy = delta*x*y - gamma*y\n",
    "    return np.array([dx, dy])\n",
    "\n",
    "# Time points\n",
    "t = np.linspace(0, 10, 1000)\n",
    "\n",
    "# Initial conditions: 10 rabbits and 5 foxes\n",
    "z0 = np.array([10.0, 5.0])\n",
    "\n",
    "# Solve the ODE\n",
    "solution = scipy_odeint(lotka_volterra, z0, t)\n",
    "\n",
    "# Convert the solution to torch tensor\n",
    "solution_torch = torch.tensor(solution, dtype=torch.float32)\n",
    "\n",
    "# Define the neural ODE\n",
    "class NeuralODE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralODE, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(2, 16),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(16, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, t, x):\n",
    "        return self.net(x)\n",
    "\n",
    "neural_ode = NeuralODE()\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(neural_ode.parameters(), lr=0.01)\n",
    "\n",
    "# Convert the numpy arrays to torch tensors\n",
    "t_torch = torch.tensor(t, dtype=torch.float32)\n",
    "z0_torch = torch.tensor(z0, dtype=torch.float32)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    out = odeint(neural_ode, z0_torch, t_torch)\n",
    "    loss = criterion(out, solution_torch)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch: {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "# Plot the solutions\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(t, solution, 'r')\n",
    "plt.plot(t, odeint(neural_ode, z0_torch, t_torch).detach().numpy(), 'b--')\n",
    "plt.legend(['True solution', 'Neural ODE solution'])\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Population Size')\n",
    "plt.title('Fit of Neural ODE to Lotka-Volterra Model')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "452270d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/3l/yn2hqvzn44x1knqzb_2538xw0000gn/T/ipykernel_43490/1752032774.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Solve the ODE using torchdiffeq's odeint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0msolution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0modeint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlotka_volterra\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Convert the solution to a numpy array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torchdiffeq/_impl/odeint.py\u001b[0m in \u001b[0;36modeint\u001b[0;34m(func, y0, t, rtol, atol, method, options, event_fn)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mevent_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0msolution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintegrate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mevent_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintegrate_until_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torchdiffeq/_impl/solvers.py\u001b[0m in \u001b[0;36mintegrate\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0msolution\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_before_integrate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0msolution\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_advance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py\u001b[0m in \u001b[0;36m_before_integrate\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_before_integrate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0mf0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst_step\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             first_step = _select_initial_step(self.func, t[0], self.y0, self.order - 1, self.rtol, self.atol,\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torchdiffeq/_impl/misc.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, t, y, perturb)\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0;31m# Do nothing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/3l/yn2hqvzn44x1knqzb_2538xw0000gn/T/ipykernel_43490/1752032774.py\u001b[0m in \u001b[0;36mlotka_volterra\u001b[0;34m(t, z, alpha, beta, delta, gamma)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Lotka-Volterra system\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlotka_volterra\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mdx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mdy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchdiffeq import odeint\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Lotka-Volterra system\n",
    "def lotka_volterra(t, z, alpha=1.0, beta=0.1, delta=0.1, gamma=1.0):\n",
    "    x, y = z[:, 0], z[:, 1]\n",
    "    dx = alpha*x - beta*x*y\n",
    "    dy = delta*x*y - gamma*y\n",
    "    return torch.stack([dx, dy], dim=1)\n",
    "\n",
    "# Time points\n",
    "t = torch.linspace(0, 10, 1000)\n",
    "\n",
    "# Initial conditions: 10 rabbits and 5 foxes\n",
    "z0 = torch.tensor([10.0, 5.0])\n",
    "\n",
    "# Solve the ODE using torchdiffeq's odeint\n",
    "solution = odeint(lotka_volterra, z0, t)\n",
    "\n",
    "# Convert the solution to a numpy array\n",
    "solution_np = solution.detach().numpy()\n",
    "\n",
    "# Convert the solution back to a torch tensor\n",
    "solution_torch = torch.tensor(solution_np, dtype=torch.float32)\n",
    "\n",
    "# Define the neural ODE\n",
    "class NeuralODE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralODE, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(2, 16),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(16, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, t, x):\n",
    "        return self.net(x)\n",
    "\n",
    "neural_ode = NeuralODE()\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(neural_ode.parameters(), lr=0.01)\n",
    "\n",
    "# Convert the time points to a torch tensor\n",
    "t_torch = torch.tensor(t, dtype=torch.float32)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    out = odeint(neural_ode, z0, t_torch)\n",
    "    loss = criterion(out, solution_torch)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch: {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "# Plot the solutions\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(t, solution_np[:, 0], 'r', label='True solution (Rabbits)')\n",
    "plt.plot(t, solution_np[:, 1], 'g', label='True solution (Foxes)')\n",
    "plt.plot(t, odeint(neural_ode, z0, t_torch).detach().numpy()[:, 0], 'b--', label='Neural ODE solution (Rabbits)')\n",
    "plt.plot(t, odeint(neural_ode, z0, t_torch).detach().numpy()[:, 1], 'm--', label='Neural ODE solution (Foxes)')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Population Size')\n",
    "plt.legend()\n",
    "plt.title('Fit of Neural ODE to Lotka-Volterra Model')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e4e4ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
